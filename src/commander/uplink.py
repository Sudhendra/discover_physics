"""
Uplink - LLM Interface for the Cognitive Commander

This module handles the communication with the LLM (OpenAI via LiteLLM).
It manages API keys, context, and parsing of the "Science Mode" decisions.
"""

import os
import json
import logging
from typing import Dict, Any, Optional

# Third-party imports
try:
    from litellm import completion
except ImportError:
    completion = None

# Local imports
from src.commander.protocols import SYSTEM_PROMPT, ANALYSIS_TEMPLATE

# Configure logging
logger = logging.getLogger(__name__)

class Commander:
    """
    The Cognitive Commander (Module 3).
    
    Acts as the Principal Investigator, reviewing hypotheses generated by the Theorist.
    Uses an LLM to determine if a scientific discovery has been made.
    """
    
    def __init__(self, model_name: str = "gpt-4o", api_key: Optional[str] = None):
        """
        Initialize the Commander.
        
        Args:
            model_name: The LLM model to use (default: gpt-4o)
            api_key: OpenAI API Key (optional, defaults to env var OPENAI_API_KEY)
        """
        self.model_name = model_name
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        
        if not self.api_key:
            logger.warning("âš ï¸ No OpenAI API Key found. Commander will be disabled.")
            
        if completion is None:
            logger.warning("âš ï¸ LiteLLM not installed. Commander will be disabled.")
            
    def review_hypothesis(self, hypothesis: Dict[str, Any], sample_size: int) -> Dict[str, Any]:
        """
        Consult the LLM to review a strong hypothesis.
        
        Args:
            hypothesis: Dictionary containing 'equation', 'r_squared', 'complexity'
            sample_size: Number of data points used for this hypothesis
            
        Returns:
            Dictionary with 'status' (DISCOVERY/CONTINUE) and 'reasoning'
        """
        if not self.api_key or completion is None:
            return {"status": "CONTINUE", "reasoning": "Commander disabled (no API key or litellm)."}
            
        equation = hypothesis.get("equation", "N/A")
        r_squared = hypothesis.get("r_squared", 0.0)
        complexity = hypothesis.get("complexity", 999)
        
        # Format the prompt
        user_prompt = ANALYSIS_TEMPLATE.format(
            equation=equation,
            r_squared=r_squared,
            complexity=complexity,
            sample_size=sample_size
        )
        
        try:
            # Call the LLM
            logger.info(f"ðŸ“¡ Transmitting hypothesis to Commander ({self.model_name})...")
            response = completion(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                api_key=self.api_key,
                temperature=0.1  # Low temperature for rigorous analysis
            )
            
            content = response.choices[0].message.content
            return self._parse_response(content)
            
        except Exception as e:
            logger.error(f"âŒ Commander Uplink Failed: {e}")
            return {
                "status": "CONTINUE", 
                "reasoning": f"Communication error: {str(e)}"
            }

    def _parse_response(self, content: str) -> Dict[str, Any]:
        """
        Parse the structured text response from the LLM.
        
        Expected format:
        REASONING: ...
        STATUS: ...
        """
        reasoning = "No reasoning provided."
        status = "CONTINUE"
        
        lines = content.strip().split('\n')
        for line in lines:
            if line.startswith("REASONING:"):
                reasoning = line.replace("REASONING:", "").strip()
            elif line.startswith("STATUS:"):
                status_text = line.replace("STATUS:", "").strip().upper()
                if "DISCOVERY" in status_text:
                    status = "DISCOVERY"
                else:
                    status = "CONTINUE"
                    
        # Fallback if multi-line reasoning
        if reasoning == "No reasoning provided." and "REASONING:" in content:
            try:
                parts = content.split("STATUS:")
                if len(parts) > 0:
                    reasoning_part = parts[0]
                    reasoning = reasoning_part.replace("REASONING:", "").strip()
            except:
                pass

        return {
            "status": status,
            "reasoning": reasoning,
            "raw_response": content
        }
